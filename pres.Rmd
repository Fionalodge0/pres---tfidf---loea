---
title: "tf-idf-pres-LAEA"
author: "Fiona R Lodge"
date: "5/12/2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and Use Case

During my internship at Securian Financial, I was given a dataset which contained medical conditions (as text) for life insurance applicants.  In the exploratory phase, I used the tf-idf statistic to extract medical conditions characteristic to each underwriting (risk) class.  As an example, diabetes may be a condition characteristic to an underwriting class associated with a higher risk.  A little bit by accident, I also discovered underwriting cases that did not follow the set-upon rules by using this statistic.  

More commonly the tf-idf statistic is used for tasks involved in information retrieval, but is also used as a text vectorization tool for predictive modeling.

## The Example Dataset

For the next few examples, I will be using the text generated from the Wikipedia articles on baseball and cricket. The tf-idf should find words that are characteristic to each sport.  

```{r tables, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(jpeg)
library(kableExtra)
library(caret)
library(tm)


cricket <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//cricket - Wikipedia.txt', header = FALSE, fill = FALSE, col.names = 'cricket.words', stringsAsFactors = FALSE)
baseball <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//Baseball - Wikipedia.txt', fill = FALSE, col.names = 'baseball.words', stringsAsFactors = FALSE)
pal <- c("#4F628E","#7887AB", "#2E4272", "#AA8439")
```

```{r image, echo=FALSE, fig.align='center', fig.width=0.25}
knitr::include_graphics('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//baseball_cricket.PNG')
```

The data was tokenized by single words using the `unnest_tokens` tool from the `tidytext` package.  A sample tokenization of the sentence 'Baseball is a bat-and-ball game' would be 'Baseball', 'is', 'a', 'bat-and-ball', 'game'.  Data cleaning steps performed included removing digits and \'s.  The data was then combined into one dataset.

```{r tokenized_data, message=FALSE, warning=FALSE}
# tokenize
tokenizer.func <- function(dat, sp = 'baseball'){
  tmp <- 
    dat %>%
    unnest_tokens(output = word, 
                  input = !!sym(colnames(dat)), 
                  token = 'words') %>%
    mutate(sport = sp) %>%
    select(sport, word) %>%
    filter(!grepl('\\d', word)) %>% # remove digits
    mutate(word = str_replace_all(word, '\'s', "")) #Stemming to remove 's
  return(tmp)
}
baseball.tokenized <- tokenizer.func(baseball)
cricket.tokenized <- tokenizer.func(cricket, sp = 'cricket')

# combine baseball and cricket
sports <- bind_rows(baseball.tokenized, cricket.tokenized)

head(sports)
```


## The Distribution of text data

Text data often follows a lognormal distribution, because the English language repeatably uses the same words, such as 'is', 'the', 'a', etc.  Below is the plot of the proportional distributions of words from both the baseball and cricket text.  

```{r dist_plot, echo=FALSE, warning=FALSE}
wordfreq <-
  sports %>%
  group_by(sport, word) %>%
  summarise(N = n()) %>%
  mutate(total = sum(N)) %>%
  ungroup()

g <- 
  wordfreq %>%
  mutate(freq = N/total) %>%
  arrange(desc(freq)) %>%
  mutate(word = factor(word, levels = unique(word))) %>%
  group_by(sport) %>%
  top_n(15, freq) %>%
  ungroup %>%
  ggplot(aes(word, freq, fill = sport)) + 
  scale_fill_manual(values = pal[c(1,4,2)])+
  geom_col(show.legend = FALSE) +
  geom_density() + 
  facet_wrap(~sport, dir = 'v', scales = 'free') 

print(suppressWarnings(g))

# Total for both together
totals <- sports %>% group_by(word) %>% summarize(N= n()) %>% arrange(desc(N)) %>% ungroup

sum <- sum(totals$N[1:6])
# totals[1:6, 'word']
```

In the combined dataframe, the words 'the', 'of', 'a', 'and', 'in', 'to' occur around 20% of the time!  These type of words are often referred to as 'stop words' and sometimes removed in the preprocessing step.

Sometimes, it may be appropriate to use frequency counts as a vector representation for text, but often it may not be an accurately describe the features.  This is the 'bag of words' vector representation.  

## Forming the idf with the natural logarithm

Frequency counts don't really tell us much about the sport of baseball and cricket, and we need to be able to differentiate characteristic words from noise.  The idf (inverse document frequency) achieves just this by decreasing its weight for words that occur more often.  See the graph below.

```{r natural_logarithm_plot, echo=FALSE}
log.df <- cbind.data.frame('x' = seq(0, 1, by = 0.001), y = log(1/seq(0, 1, by = 0.001)))

ggplot(log.df, 
       aes(x = x, y = y)) + 
       geom_line() + 
  xlab('Total number of Documents/Number of Documents Containing That Word') +   ylab('IDF (log(x))') + 
  ggtitle('IDF')
```

For example, since the word 'the' occurs in both documents, `idf: log(2/2) = 0`. However, since the word 'cricket' occurs in only one of the documents, `idf: log(2/1) = 0.69`.  Hence, the words that are unique to each document will receive a higher weight than those that occur frequently.  

## The tf-idf statistic

Term frequency is still important, i.e. if baseball occurs once in a document it may not be as much about baseball as a document that contains the word baseball 50 times.  So, we have arrived at the full version of the tf-idf statistic:

Insert here $ \sqrt(2) $

Variations include:
- various normalization techniques for the idf, including addition of 1 to the logarithm to avoid division by 0.
- squaring of the statistic in practice (?)
- normalization of the tf statistic to avoid bias on text length

## Using the tf-idf statistic on the cricket/baseball example

Below are the results of calculating the `tf-idf` on the cricket/baseball data, using the `bind_tf_idf` function in the `tidytext` package. 

```{r tf_idf_basball_data}
sports_tfidf <- 
  wordfreq %>%
  bind_tf_idf(term = word, document = sport, n = N)

sports_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(sport) %>%
  top_n(15, tf_idf) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = sport)) + 
  scale_fill_manual(values = pal[c(1,2,4)])+
  geom_col(show.legend = FALSE) +
  facet_wrap(~sport, scales = 'free') + 
  coord_flip() 
```

This is much more informative!  For example, I now know that bowler is to cricket as pitcher is to baseball.  From the 'american' and 'mlb', I can guess that baseball is popular in America, but 'icc' and 'nations' suggests that cricket is more popular as an international sport.  From an exploratory sense, this is much more informative.  

## Simple Information Retrieval

In its simplest form an information retrieval engine will sum up the tf_idf of the words in the query. 
```{r recomender}
query <- c('What', 'sport', 'is', 'played', 'on', 'a', 'diamond')

sports_tfidf %>%
  group_by(sport) %>%
  filter(word %in% query) %>%
  summarize(tfidf_sum = sum(tf_idf))
```

## Various methods for text vectorization

There are several commands in both R and Python that will produce a Document Term Matrix with tf-idf weightings.  Something to note is that often these matrices take a term frequency vector instead of a vector of text.  Below is a list and notes on a few of them.  As is typical of R, there is probably a plethora of functions to do the same thing, and conversely in Python, one function with a plethora of options.

R:
- `DocumentTermMatrix` or `TermDocumentMatrix`.  I didn't try this one as my data was in a dataframe and not structured as a `corpus`.  It does have a control function to choose the tf-idf weightings (`weightTfIdf`) with the option to normalize the term frequencies. 
- `cast_dtm`.  Better option if you have a tidy dataframe of terms and don't want to work through generating a corpus structure.  Requires a count column (`value`).  The tf-idf weighting is `weighting = tm::weightTfIdf` and can normalize the term frequency. 

Python:
- `TfIdfTransformer`. Contains a couple idf normalization techniques, and a smoother.  This function merely transforms the matrix, which means you will have to form the frequency counts first, `CountVectorizer` can achieve this.  This function is useful if you want to take a more step-by-step approach.
- `TfIdfVectorizer`.  Performs `CountVectorizer` and `TfIdfTransformer` at once, also has some preprocessing options.


## Pitfalls of tf-idf

The tf-idf statistic pays little attention to relevancy/context.  For example, for the query, 'unique red hat', I can game a simple system.  A user may try this approach if they want their product to appear more often in a search result.  

```{r system_game}
table <- tibble('Document' = c('doc1', 'doc2','doc3'), 'Results' = c('unique, unique, unique, unique, purple, purple, shirt', 'I tipped my cap to the red team', 'This red hat sells for cheap at JcPenny.'))
table2 <- 
  unnest_tokens(table, output = 'word', input = Results) %>%
  group_by(Document, word) %>%
  summarise(N = n()) %>%
  ungroup() %>%
  bind_tf_idf(term = word, document = Document, n = N) %>%
  filter(word %in% c('unique', 'red', 'hat')) %>%
  group_by(Document) %>%
  summarize(sum.tfidf = sum(tf_idf)) %>%
  select(sum.tfidf)
  
kable(table %>% bind_cols(table2), format = 'markdown')
```

In this context, hat is probably more important than the word unique, but the tf-idf ignores this relevancy.

## Use-case - Document Similarity

Since the tf-idf extracts the characteristic words for a document, it is also useful in identifying the similarity between documents.  As a use-case, if a company uses common language in its documents, the resulting idf would be low, essentially labeling those documents as 'typical'.  Put through some type of similarity measure/clustering model, this will result in a strong relationship to the similar documents.   

I have been coaching high school softball for 6 years, and I was curious to know if I have started to repeat myself.  As an example, I extracted two different emails about tryouts and one spam email. I found the tf-idf and then calculated the cosine similarity for each document.  My emails are not similar (they should have a ratio close to 1) - but the spam email is not similar at all!  

```{r email_files, include=FALSE}
tryout2019 <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//tryout-notes.txt', header = FALSE, fill = FALSE, col.names = 'tryout2019', stringsAsFactors = FALSE)
spam <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//spam-email.txt', header = FALSE, fill = FALSE, col.names = 'spam', stringsAsFactors = FALSE, quote="")
tryout2018 <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//Tryouts-2018.txt', header = FALSE, fill = FALSE, col.names = 'tryout2018', stringsAsFactors = FALSE)

tok1 <- tokenizer.func(tryout2019, sp = 'tryouts2019')
tok2 <- tokenizer.func(tryout2018, sp = 'tryouts2018')
tok3 <- tokenizer.func(spam, sp = 'spam')

emails <- bind_rows(tok1, tok2, tok3)

df <- 
  emails %>%
  count(sport, word) %>%
  cast_dtm(term = word, document = sport, value = n, weighting = tm::weightTfIdf)

library(lsa)
mat <- cosine(t(as.matrix(df))) 

```

```{r mat}
print(mat)
```

## My hunches

## How does Google search?

There it is, but if I search tf idf, I can no longer find it. 

## Citations

[1] Silge, J., & Robinson, D. (2019, March 23). Text Mining with R. Retrieved from https://www.tidytextmining.com/

[2] LuceneSolrRevolution. (2013, May 29). Beyond TF-IDF: Why, What and How. Retrieved from https://www.youtube.com/watch?v=C25txE_dq90

[3] Tf–idf. (2019, April 25). Retrieved from https://en.wikipedia.org/wiki/Tf–idf
