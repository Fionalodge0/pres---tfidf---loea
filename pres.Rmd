---
title: "tf-idf-pres-LAEA"
author: "Fiona R Lodge"
date: "5/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and Use Case

During my internship at Securian, I was given a dataset which contained medical conditions (as text) for life insurance applicants.  In the exploratory phase, I used the tf-idf statistic to extract medical conditions characteristic to each underwriting (risk) class.  As an example, diabetes may be a condition characteristic to an underwriting class associated with a higher risk.  A little bit by accident, I also discovered underwriting cases that did not follow the set-upon rules by using this statistic.  

Most commonly the tf-idf statistic is used for tasks involved in information retrieval, but is also used as a text vectorization tool for predictive modeling.  
## The First Dataset

For the next few examples, I will be using the text generated from the Wikipedia articles on baseball and cricket. Although these sports are considered similar, the tf-idf should pull words that are characteristic to each sport.  

```{r tables, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(jpeg)
library(kableExtra)
library(caret)
library(tm)


cricket <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//cricket - Wikipedia.txt', header = FALSE, fill = FALSE, col.names = 'cricket.words', stringsAsFactors = FALSE)
baseball <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//Baseball - Wikipedia.txt', fill = FALSE, col.names = 'baseball.words', stringsAsFactors = FALSE)
pal <- c("#4F628E","#7887AB", "#2E4272", "#AA8439")
```

```{r image, echo=FALSE, fig.align='center', fig.width=0.25}
knitr::include_graphics('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//baseball_cricket.PNG')
```

The data was tokenized by single words using the `unnest_tokens` tool from the tidytext package.  A tokenization of the sentence 'Baseball is a bat-and-ball game' would be 'Baseball', 'is', 'a', 'bat-and-ball', 'game'.  

```{r tokenized_data, message=FALSE, warning=FALSE}
# tokenize
tokenizer.func <- function(dat, sp = 'baseball'){
  tmp <- 
    dat %>%
    unnest_tokens(output = word, input = !!sym(colnames(dat)), token = 'words') %>%
    mutate(sport = sp) %>%
    select(sport, word) %>%
    filter(!grepl('\\d', word)) %>% # remove numbers
    mutate(word = str_replace_all(word, '\'s', ""))
  return(tmp)
}
baseball.tokenized <- tokenizer.func(baseball)
cricket.tokenized <- tokenizer.func(cricket, sp = 'cricket')

# combine baseball and cricket
sports <- bind_rows(baseball.tokenized, cricket.tokenized)

head(sports)
```

The same was done for the text data for the sport of cricket and row-binded into one dataframe.  

## The Distribution of text data

Text data often follows a lognormal (?) distribution, mainly because the english language repeatly uses the same words, such as 'is', 'the', 'a', etc.  Below is the plot of the proportional distributions of words from both the baseball and cricket text.  


```{r dist_plot, echo=FALSE, warning=FALSE}
wordfreq <-
  sports %>%
  group_by(sport, word) %>%
  summarise(N = n()) %>%
  mutate(total = sum(N)) %>%
  ungroup()

g <- 
  wordfreq %>%
  mutate(freq = N/total) %>%
  arrange(desc(freq)) %>%
  mutate(word = factor(word, levels = unique(word))) %>%
  group_by(sport) %>%
  top_n(15, freq) %>%
  ungroup %>%
  ggplot(aes(word, freq, fill = sport)) + 
  scale_fill_manual(values = pal[c(1,4,2)])+
  geom_col(show.legend = FALSE) +
  geom_density() + 
  facet_wrap(~sport, dir = 'v', scales = 'free') 

print(suppressWarnings(g))

# Total for both together
totals <- sports %>% group_by(word) %>% summarize(N= n()) %>% arrange(desc(N)) %>% ungroup

sum <- sum(totals$N[1:6])
# totals[1:6, 'word']
```

In the combined dataframe, the words 'the', 'of', 'a', 'and', 'in', 'to' occur around 20% of the time!  These type of words are often referred to as 'stop words' and sometimes removed in the preprocessing step.  

Sometimes, it may be appropriate to use frequency counts as a vector representation for text, but often it may not be an accurate depictor of the textual features.  This is the 'bag of words' vector representation of text.  

## Forming the idf with the natural logarithm

Frequency counts don't really tell us much about the sport of baseball and cricket, and we need to be able to differentiate characteristic words from noise.  The idf (inverse document frequency) achieves just this by decreasing its weight for words that occur more often.  See the graph below

```{r natural_logarithm_plot}
log.df <- cbind.data.frame('x' = seq(0, 1, by = 0.001), y = log(1/seq(0, 1, by = 0.001)))

ggplot(log.df, aes(x = x, y = y)) + geom_line() + xlab('Total number of Documents/Number of Documents Containing That Word') + ylab('log(x)') + ggtitle('IDF')
```

For example, since the word 'the' occurs in both documents, `idf: log(2/2) = 0`. However, since the word 'cricket' occurs in only one of the documents, `idf: log(2/1) = 0.69`.  Hence, the words that are unique to each document will recieve a higher weight than those that occur frequently.  

## The tf-idf statistic

Term frequency is still important, i.e. if baseball occurs once in a document it may not be as much about baseball as a document that contains the word baseball 50 times.  So, we have arrived at the full version of the tf-idf statistic:

Insert here $ \sqrt(2) $

Variations include:
- various normalization techniques for the idf, including addition of 1 to the logarithm to avoid division by 0.
- squaring of the statistic in practice (?)
- normalization of the tf statistic to avoid bias on text length

## Using the tf-idf statistic on the cricket/baseball example

Below are the results of calculating the `tf-idf` on the cricket/baseball data. 

```{r tf_idf_basball_data}
sports_tfidf <- 
  wordfreq %>%
  bind_tf_idf(term = word, document = sport, n = N)

sports_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(sport) %>%
  top_n(15, tf_idf) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = sport)) + 
  scale_fill_manual(values = pal[c(1,4,2)])+
  geom_col(show.legend = FALSE) +
  facet_wrap(~sport, scales = 'free') + 
  coord_flip() 
```

This is much more informative!  For example, I now know that bowler is to cricket as pitcher is to baseball.  From the 'american' and 'mlb', I can guess that baseball is popular in America, but 'icc' and 'nations' suggests that cricket is more popular as an international sport.  From an exploratory sense, this is much more informative.  

## Simple Information Retrieval

In its simplest form an information retrieval engine will sum up the tf_idf of the words in the query. 
```{r recomender}
query <- c('What', 'sport', 'is', 'played', 'on', 'a', 'diamond')

sports_tfidf %>%
  group_by(sport) %>%
  filter(word %in% query) %>%
  summarize(tfidf_sum = sum(tf_idf))
```

## Common Recomendation Engine: tf_idf with cosine similarity



```{r cosine_sim}
 

```

## Various methods for text vectorization

There are several commands in both R and Python that will produce a Document Term Matrix with tf-idf weightings.  Something to note is that often these matrices take a term frequency vector instead of a vector of text.  Below is a list and notes on a few of them.  As is typical of R, there is probably a plethora of functions to do the same thing, and conversely in python, one function with a plethora of options.

R:
- `DocumentTermMatrix` or `TermDocumentMatrix`.  I didnt try this one as my data was in a dataframe and not structured as a `corpus`.  It does have a control function to choose the tf-idf weightings (`weightTfIdf`) with the option to normalize the term frequencies. 
- `cast_dtm`.  Better option if you have a tidy dataframe of terms and don't want to work through generating a corpus structure.  Requires a count column (`value`).  The tf-idf weighting is `weighting = tm::weightTfIdf` and can normalize the term frequency. 

## 

Python:
- `TfIdfTransformer`


## Pitfalls of tf-idf

The tf-idf statistic pays little attention to relevancy/context.  For example, for the query, 'unique red hat', I can game a simple system.  A user may try this approach if they want their results to appear more often.  

```{r system_game}
table <- tibble('Document' = c('doc1', 'doc2','doc3'), 'Results' = c('unique, unique, unique, unique, purple, purple, shirt', 'I tipped my cap to the red team', 'This red hat sells for cheap at JcPenny.'))
table2 <- 
  unnest_tokens(table, output = 'word', input = Results) %>%
  group_by(Document, word) %>%
  summarise(N = n()) %>%
  ungroup() %>%
  bind_tf_idf(term = word, document = Document, n = N) %>%
  filter(word %in% c('unique', 'red', 'hat')) %>%
  group_by(Document) %>%
  summarize(sum.tfidf = sum(tf_idf)) %>%
  select(sum.tfidf)
  
kable(table %>% bind_cols(table2), format = 'markdown')
```

In this context, hat is probably more important than the word unique, but the tf-idf ignores this relevancy.

## Usecase - Document Similarity

Since the tf-idf extracts the characteristic words for a document, it is also useful in identifying the similarity between documents.  As a usecase, if a company uses common language in its documents, the resulting idf would be low, essentially labelling those documents as 'typical'.  Put through sometype of similarity measure/clustering model, this will result in a strong relationship to the similiar documents.   

I have been coaching high school softball for 6 years, and I was curious to know if I have started to repeat myself and sound similar.  As an example, I extracted two different emails about tryouts and one spam email. I found the tf-idf and then calculated the cosine similarity for each document.  My emails are not similar (they should have a ratio close to 1) - but the spam email is not similar at all!  

```{r email_files, include=FALSE}
tryout2019 <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//tryout-notes.txt', header = FALSE, fill = FALSE, col.names = 'tryout2019', stringsAsFactors = FALSE)
spam <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//spam-email.txt', header = FALSE, fill = FALSE, col.names = 'spam', stringsAsFactors = FALSE, quote="")
tryout2018 <- read.delim2('C://Users//Owner//Documents//Github//tf_idfpresentationonbaseballandcricket//My Emails//Tryouts-2018.txt', header = FALSE, fill = FALSE, col.names = 'tryout2018', stringsAsFactors = FALSE)

tok1 <- tokenizer.func(tryout2019, sp = 'tok1')
tok2 <- tokenizer.func(tryout2018, sp = 'tok2')
tok3 <- tokenizer.func(spam, sp = 'spam')

emails <- bind_rows(tok1, tok2, tok3)

df <- 
  emails %>%
  count(sport, word) %>%
  cast_dtm(term = word, document = sport, value = n, weighting = tm::weightTfIdf)

library(lsa)
mat <- cosine(t(as.matrix(df))) 
```

